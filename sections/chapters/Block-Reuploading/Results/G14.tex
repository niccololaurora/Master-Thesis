\begin{figure}[h]
    \centering
    \includegraphics[scale=0.3]{sections/chapters/Block-Reuploading/Images/Heatmaps/Global-Heatmaps/Heatmap-Digits-14x14-G.pdf}
    \caption{This heatmap shows the training and validation accuracy for the $14\times14$ 
    down-scaled MNIST digits dataset using a global Pauli Z.}
    \label{fig:heatmap-14x14-G}
\end{figure}


By enlarging the images to $14\times14$, we can again notice a \textit{shift to the left by 
one or two layers} of the validation accuracy with respect to the $12\times12$ local case (figure 
\ref{fig:heatmap-14x14-G}). Once again, this shift 
occurs because the increased number of parameters in each layer makes the model more prone to overfitting.
The training accuracy for the $14\times14$ is essentially the same for both the $8\times8$ and 
$12\times12$ cases.

The remarks regarding the \textit{Deep-Narrow}, \textit{Shallow-Wide}, and \textit{Proportionate} 
architectures are consistent with those made in Section \ref{sssec:local-12} for the $12\times12$ case.
By examining Figure \ref{fig:heatmap-14x14-G}, no new behavior is observed that hasn't already 
been discussed in the $8\times8$ and $12\times12$ comparisons.