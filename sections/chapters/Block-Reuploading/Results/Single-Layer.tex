

As observed in subsection \ref{sssec:num1} and illustrated by the first column of the heatmap 
\ref{fig:heatmap-8x8-L}, there is a notable decrease in accuracy as the architectures become wider.\\
This behavior is not observed in the global case: as shown in heatmap \ref{fig:heatmap-8x8-G}, 
the accuracy remains stable as the architectures become wider, without the same significant drop.
This difference between the local and global cases can be attributed to the \textit{entanglement dynamics} and 
the \textit{type of observable used for decoding}. 
In the local case, the lack of entanglement limits information sharing across qubits. 
In contrast, a global observable, which aggregates information from all qubits, mitigates the 
impact of limited information sharing in wider architectures.\\

This intriguing behavior warrants a detailed discussion. 
In this section, we will analyze the evolution of loss functions and accuracies over 200 epochs, 
as illustrated in figure \ref{fig:single}.

\begin{itemize}
    \item \textbf{Training Loss}.\\ 
    An examination of the global and local training losses reveals that the 1-qubit configuration 
    has the lowest loss, with losses increasing as the number of qubits 
    increases\footnote[1]{This observation holds true except for the global 11-qubit configuration, 
    which appears as an outlier. This anomaly could be addressed by conducting a statistical analysis 
    with multiple training runs using different initialization seeds.}.
    As the number of qubits increases, the architecture becomes more challenging to train, resulting 
    in a higher loss function plateau in performance.
    \item \textbf{Validation Loss}.\\ 
    For validation loss, the trend of loss decreasing with the number of qubits is consistent 
    in the local case but not in the global case, where the losses are mixed.
    \item \textbf{Training-Validation Accuracy}.\\
    The local accuracy exhibits numerical instability as the number of qubits increases. 
    Initially smooth, the accuracy becomes increasingly "stair-stepped" with more qubits, 
    and eventually levels off, becoming constant for 14-qubit and 15-qubit architectures. 
    This progression illustrates how numerical instability becomes more pronounced with a 
    larger number of qubits.\\
    We can assert that the local 9-qubits (or 8-qubits) architecture represents the threshold beyond which numerical 
    instability becomes more pronounced.\\
    It is noteworthy that \textit{numerical instability is not observed in the global case}. 
    A possible explanation is that a global observable aggregates more information, which may 
    help mitigate the numerical instability seen when measuring an individual qubit.
\end{itemize}

Does the numerical instability persist with additional layers? 
By increasing the number of layers and generating the same plots, we observe that the numerical 
instability vanishes (figure \ref{fig:three-layers} shows architectures with three layers).

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[scale=0.45]{sections/chapters/Block-Reuploading/Images/Single-Layer/single-layer-comparison-compact-local-8x8-layer1.pdf}
    \caption{This figure shows the training and validation loss and accuracy for \textbf{single-layer} architectures
    trained on the $8\times8$ down-scaled MNIST digits dataset using a \textbf{local} Pauli Z.}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[scale=0.45]{sections/chapters/Block-Reuploading/Images/Single-Layer/single-layer-comparison-compact-global-8x8-layer1.pdf}
    \caption{This figure shows the training and validation loss and accuracy for \textbf{single-layer} architectures
    trained on the $8\times8$ down-scaled MNIST digits dataset using a \textbf{global} Pauli Z.}
    \end{subfigure}
    \caption{This comparison reveals that for \textbf{single layer} architectures in the local case, a numerical instability causes the accuracy 
    to exhibit a pronounced 'stair-step' pattern as the number of qubits increases, eventually 
    leveling off and remaining constant for 14- and 15-qubit architectures. Notably, this numerical 
    instability is absent in the global case.}
    \label{fig:single}
\end{figure}


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[scale=0.45]{sections/chapters/Block-Reuploading/Images/Single-Layer/single-layer-comparison-compact-local-8x8-layer3.pdf}
    \caption{This figure shows the training and validation loss and accuracy for \textbf{3-layers} architectures
    trained on the $8\times8$ down-scaled MNIST digits dataset using a \textbf{local} Pauli Z.}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[scale=0.45]{sections/chapters/Block-Reuploading/Images/Single-Layer/single-layer-comparison-compact-global-8x8-layer3.pdf}
    \caption{This figure shows the training and validation loss and accuracy for \textbf{3-layers} architectures
    trained on the $8\times8$ down-scaled MNIST digits dataset using a \textbf{global} Pauli Z.}
    \end{subfigure}
    \caption{This comparison reveals that for architectures with more than one layer (3 layers in this plots) 
    in the local case and in the global case there's no sign of the numerical instability witnessed in the single layer
    case.}
    \label{fig:three-layers}
\end{figure}

