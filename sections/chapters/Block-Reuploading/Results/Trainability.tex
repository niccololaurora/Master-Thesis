\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{sections/chapters/Block-Reuploading/Images/Trainability/variance-qubits-glob-loc.pdf}
    \caption{The first row compares the \textit{$Var(|\nabla_{\bm{\theta}}J|)$ vs layers} plots for the local (left)
    and global (right) case with a uniform initialization of the parameters ranging [-$\pi$, $\pi$].
    The first row compares the \textit{$Var(|\nabla_{\bm{\theta}}J|)$ vs qubits} plots for the local (left)
    and global (right) case with a uniform initialization of the parameters ranging [-$\pi$, $\pi$].
    By local and global case we intend that we either used local Pauli Z or global Pauli Z, as the 
    decoding method.}
    \label{fig:uniform-init}
\end{figure}

We investigated the \textit{trainability} (figures \ref{fig:uniform-init} and \ref{fig:local-gauss}) 
of the \textit{block re-uploading} architecture following the procedure introduced by 
the foundational paper by \cite{McClean_2018}.\\
Our methodology is the following:

\begin{itemize}
    \item We fixed the size of the images: $8\times8$;
    \item We fixed the number of layers: 1, 10, 15, 20, 40, 60, 80, 100;
    \item We fixed the number of qubits: 1, 2, 3, ..., 16;
    \item We sampled N (50) models, which means that we sampled N different times the parameters 
    $\bm{\theta}$ of the model (uniform and gaussian initialization);
    \item We computed the absolute gradient of the loss function $|\nabla_{\bm{\theta}}J|$\footnote[1]{
        Alternatively, we could have computed only the loss function. As previously discussed, 
        barren plateaus can be defined either through the loss function or its gradient. 
        This approach would have been computationally more efficient.
    };
    \item We computed the variance value of the absolute gradient $Var(|\nabla_{\bm{\theta}}J|)$;
\end{itemize}

Our approach differs from the original paper on barren plateaus, as \cite{McClean_2018} 
used architectures with only parameters in their parametric gates, without data embedding. 
In contrast, the \textit{block re-uploading} architecture incorporates both data and parameters 
in every parametric gate.\\

We studied the trainability of the \textit{block re-uploading} architecture with two different decodings:
local Pauli Z and global Pauli Z.\\ 
Let's compare these two decodings technique when initializing the trainable parameters uniformly 
between [-$\pi$, $\pi$] (figure \ref{fig:uniform-init}):

\begin{enumerate}
    \item \textit{$Var(|\nabla_{\bm{\theta}}J|)$ vs layers}: Global vs Local.\\
    By comparing the two decoding scenarios, we can notice a common behaviour:
    initially, there is an exponential decrease in variance for the first few layers (which is more pronounced 
    in the local case), after which the variance levels off and reaches a plateau.
    The primary difference between the global and local cases is that in the global case, 
    the plateau is reached more quickly.\\
    It is particularly noteworthy that as the number of qubits increases, 
    the plateaus in both the global and local cases become progressively lower.
    This behaviour indicates that the variance is inversely proportionate to the number of qubits.\\
    Finally, we can examine the architectureâ€™s \textit{layers budget} before reaching barren 
    plateaus\footnote[1]{By \textit{layers budget}, we refer to the number of layers an architecture 
    can accommodate before encountering a barren plateau. For this analysis, we define the 
    threshold for a barren plateau as $10^{-4}$}.\\
    In the local case, every qubit architecture maintains 
    a non-zero \textit{layers budget} (L1). For architectures with fewer qubits, this budget is effectively 
    unlimited as the plateau is above $10^{-4}$ (L2). However, as the number of qubits increases, 
    the layers budget decreases, with the 16-qubit architecture having a layers budget ranging from 
    0 to 8 (L3)\footnote[2]{I have indicated with (L1), (L2), (L3) to express that those three 
    observations are valid only for the local case.}. 
    In contrast, the global case shows a different behavior: architectures with 14, 15, and 
    16 qubits have no \textit{layers budget}, indicating that they are almost untrainable.\\
    Moreover, the local and global cases reveals distinct behaviors for single-layer architectures: 
    in the local case, the variance for different qubit architectures is concentrated within the range 
    [$10^{-3}$, $10^{-1}$]; in contrast, for the global case, as the number of qubits increases, 
    the variance progressively decreases, ranging from [$10^0$, $10^5$].

    \item \textit{$Var(|\nabla_{\bm{\theta}}J|)$ vs qubits}: Global vs Local.\\
    Since both plots are in log scale, by looking at 
    \textit{$Var(|\nabla_{\bm{\theta}}J|)$ vs qubits} we can notice that as the number of layers increases 
    $Var(|\nabla_{\bm{\theta}}J|)$ is exponentially suppressed\footnote[1]{A straight-line function 
    with a negative slope on a logarithmic scale corresponds to a negative exponential function 
    on a linear scale.}. 
    In both cases (global and local), we observe that the variance 
    is inversely correlated with the number of qubits: as the number of qubits increases, 
    the variance decreases.
\end{enumerate}


After examining \textit{uniform initialization} (see figure \ref{fig:local-uniform}), we evaluated 
\textit{gaussian initialization} (shown in figure \ref{fig:local-gauss}) for local decoding. However, 
we observed no significant differences between the two initialization methods.


\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{sections/chapters/Block-Reuploading/Images/Trainability/variance-qubits-layer-local-uniform.pdf}
    \caption{The figure shows the \textit{$Var(|\nabla_{\bm{\theta}}J|)$ vs layers} and
    \textit{$Var(|\nabla_{\bm{\theta}}J|)$ vs qubits} plots with a local decoding
    and a gaussian initialization of the parameters ($\mu = 1$, $\sigma = 1$).}
    \label{fig:local-gauss}
\end{figure}