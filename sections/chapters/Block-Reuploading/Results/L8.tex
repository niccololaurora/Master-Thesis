
By looking at figure \ref{fig:heatmap-8x8-L}, we can distinguish 
three different behaviours:

\begin{enumerate}
    \item \textbf{Deep-Narrow}.\\
    As the number of layers increases in narrow architectures (1, 2, or 3 qubits), their capacity to 
    generalize diminishes, leading to overfitting. This occurs because the increase in layers corresponds 
    to a rise in the number of trainable parameters (see figure \ref{arc:embed-pooling}).\\
    As a result, the architecture becomes overparameterized, allowing it to capture even minor 
    fluctuations in the training dataset, which reduces its ability to generalize effectively to new, 
    unseen data.
    \item \textbf{Shallow-Wide}.\\
    As the number of qubits (width) increases both training and validation accuracy of 
    single-layer architectures (shallow) drop drastically.
    As the width of the architecture increases, a greater degree of entanglement is required to 
    effectively distribute information across all qubits. However, shallow architectures lack 
    sufficient entangling gates to achieve this, resulting in an inability to fully capture and 
    “understand" the complete picture.\\
    However, by comparing training and validation accuracy, although both decrease as 
    the width increases, there is no evidence of overfitting.\\
    We can conclude that, as the architecture widens, entanglement becomes increasingly crucial for the 
    architecture to effectively ”understand” the image.
    \item \textbf{Proportionate}.\\
    In proportionate architectures (those that are neither shallow-wide nor deep-narrow) overfitting 
    tends to disappear. As the number of layers and qubits increases, the \textit{block re-uploading} 
    introduces a richer set of entanglement structures, enabling more effective information sharing 
    across all qubits. This reduction in overfitting as entanglement grows raises some natural questions:
    \textit{Could entanglement be a factor that resists overfitting?}, 
    \textit{Might entanglement serve as a source of regularization?}.
\end{enumerate}


\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{sections/chapters/Block-Reuploading/Images/Heatmaps/Local-Heatmaps/Heatmap-Both-8x8-L.pdf}
    \caption{This heatmap shows the training and validation accuracy for architectures with 1-15 qubits and
    1-6 layers for the $8\times8$ down-scaled MNIST digits and fashion dataset using a local Pauli Z.}
    \label{fig:heatmap-8x8-L}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[scale=0.45]{sections/chapters/Block-Reuploading/Images/Sfera-Bloch/q1-l6-sphere-before.pdf}
        \caption{Before training.}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[scale=0.45]{sections/chapters/Block-Reuploading/Images/Sfera-Bloch/q1-l6-sphere-after.pdf}
        \caption{After training.}
    \end{subfigure}
    \caption{This figure illustrates the distribution of quantum states for an architecture with 
    1 qubit and 6 layers, both before and after training, in the task of classifying the digits 
    0 and 1 from the MNIST dataset. Before training, the predictions are uniformly distributed 
    across the Bloch sphere. However, after training, the architecture clearly distinguishes between 
    the zeros and ones, demonstrating its improved classification capability.}
    \label{fig:sfere-A-B}
\end{figure}