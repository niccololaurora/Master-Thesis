\chapter{Quantum Machine Learning}

Quantum Machine Learning (QML) explores the interplay of ideas from quantum computing and
machine learning~\cite{Biamonte_2017}.
For example, QML investigates whether quantum computers can speed up the time it takes
to train or evaluate a machine learning model. On the other hand, the QML community
leverages techniques from machine learning to devise new quantum error-correcting codes~\cite{Roffe_2019}, 
estimate the properties of quantum systems, or develop new quantum algorithms.\\



The state of the art of Quantum Machine Learning has as main highlight the development
of algorithms that have been proven to have a quantum advantage in computational
complexity over classical algorithms~\cite{Sweke_2021, Liu_2021, Jerbi_2021}.
However, these algorithms require a fault-tolerant quantum computer~\cite{shor}, which is able to
continuously correct errors that arise during computation, ensuring the stability and
reliability of the quantum processes over extended periods.
As we are still many years away from fault tolerant quantum computation, the QML
community has developed a great interest toward possible applications of current and
near-term quantum devices (NISQ devices)~\cite{Preskill_2018}, which are not capable of continuous
quantum error correction.
In particular, the QML research community has developed a new class of quantum
procedures called variational quantum algorithms (VQA)~\cite{Cerezo_2021} to take advantage of current
and near-term quantum hardware.\\
However no quantum advantage has been proven for this algorithms, yet.\\

As previously stated, QML is the intersection of Machine Learning and Quantum Computing and this intersection can be 
factorised in four main areas:


\begin{itemize}
    \item \textbf{Classical for Classical (CC)}.\\ This area actually do not have a quantum component and 
    it indicates those purely classical cases when a classical machine learning algorithm is used to solve 
    a classical task.
    \item \textbf{Classical for Quantum (CQ)}.\\ This area indicates those studies which aim to use classical
    machine learning procedures to deal with problems in the quantum physics domain.
    \item \textbf{Quantum for Classical (QC)}.\\ This area instead refers to using quantum resources or algorithms, 
    for example variational quantum algorithms, to analyse or process classical information, 
    that is data that come from a classical source.
    \item \textbf{Quantum for Quantum (QQ)}.\\ This last area is arguably the most compelling yet unexplored
    application of Quantum Machine Learning, regarding the use of quantum processors to
    learn or study properties of quantum systems.
\end{itemize}

In my Master's thesis I have focused only on the \textit{Quantum for Classical} area.\\


In the next paragraphs I will briefly discuss Variational Quantum Algorithms, as they have been the focus
of my Master's thesis.


\section{Linear quantum models: explicit and implicit models}


\subsection{Explicit models}

A quantum machine learning model deals with data, thus the first step is to efficiently upload the data of the 
problem into the model.
Data can be either \textit{classical} $\{\bm{x}_i\}_{i=1}^m \in \chi \subseteq \mathbb{R}$ 
or \textit{quantum}, such as a set of states $\{ |\psi_i\rangle \}_{i=1}^m \in \mathcal{H} = C^{2n}$, 
where $\mathcal{H}$ is the Hilbert space of a quantum system of n qubits.\\
Let's discuss separately how to upload classical and quantum data on a quantum computer.

\begin{enumerate}
    \item Classical data.\\
    The procedure that uploads classical data $\{\bm{x}_i\}_{i=1}^m$ on a quantum computer is called
    \textit{feature embedding}.
    Specifically, this is achieved by a unitary $U(\bm{x})$ which depends on the data:

    \begin{align}
        \phi : \chi &\rightarrow \mathcal{F} = \mathcal{H}\\
        \bm{x} &\mapsto |\phi(\bm{x})\rangle = U(\bm{x}) |0\rangle^{\otimes n}\\
    \end{align}

    where $\mathcal{F}$ is the feature space, which in this case is the $\mathcal{H}$.
    We have numerous different feature embedding maps $U(\bm{x})$: \textit{angle encoding}, \textit{amplitude encoding},
    \textit{Qsample encoding}.\\
    For example the angle encoding consists in a different Pauli rotation for each component of the datapoint
    (thus requiring one qubit per dimension of the input data $d = n$):

    \begin{align}
        U(\bm{x}) = \otimes_{i=1}^n R_{P_i}(x_i) 
        \qquad 
        \bm{x} = (x_1, x_2, ..., x_N), P_i = {X, Y, Z}
    \end{align}

    In the amplitude encoding each component of a datapoint of dimension d becomes the amplitude of a vector 
    of the computational basis, thus requiring a number of qubit $n = \left\lceil log_2(d) \right\rceil$
    \footnote[1]{
        The amplitude encoding requires that the components of each datapoint $\bm{x}$ must be normalized. 
        This a fundamental restriction because 
        this means that quantum states represent the data one less dimension or with one less degree of freedom.
        For example a classical two dimensional vector $(x_1, x_2)$ can only be associated with an amplitude 
        vector $(\alpha_1, \alpha_2)$ of a qubit which fulfils $|\alpha_1|^2 + |\alpha_2|^2 = 1$.
    }.

    \begin{align}
    \bm{x} = \begin{pmatrix}
        x_1 \\
        x_2 \\
        ... \\
        x_{2^n} \\
    \end{pmatrix}
    \qquad
    \rightarrow
    \qquad
    \psi = \sum_i^{2^n} x_j | j \rangle
    \end{align}

    \item Quantum data.\\
    If we want to upload a certain quantum state $|\psi_i\rangle$ on a quantum computer, we have to determine
    the appropriate unitary $U_i$, which applied to the ground state gives $|\psi_i\rangle$:
    
    \begin{equation}
        U_i |0\rangle^{\otimes n} = |\psi_i\rangle
    \end{equation}
\end{enumerate}

Suppose we are given a classical dataset, thus we have to map these datapoints to an appropriate Hilbert space

\subsection{Implicit models}


\section{Variational Quantum Algorithms}

Variational Quantum Algorithms~\cite{Cerezo_2021} are the leading proposal to exploit current quantum computing platforms, 
based on the hope that it is possible to achieve a meaningful quantum advantage already in the non error-corrected 
regime, before standard quantum algorithms, like Shor’s factoring algorithm~\cite{Shor_1997}, can be realised at scale.\\ 

A VQA is a \textit{hybrid quantum-classical} algorithm with different components:

\begin{enumerate}
    \item \textbf{Encoding block}.\\
    The first component is the encoding block, which is responsible to encode the data of the problem in the 
    algorithm.
    \item \textbf{Parametrized Quantum Circuit (PQC)}.\\
    The second component is a parametrized quantum circuit U($\theta$), which is a circuit made of numerous gates. The parameters are usually 
    the angles of the circuit and they can be tuned to enhance the algorithm's performance for completing a specific task. 
    \item \textbf{Decoding Block}.\\
    The third component is the decoding block, which consists in measuring the PQC.
    \item \textbf{Optimization}.\\
    The valuable information extracted when measuring the PQC is used to evaluate a objective function.
    This objective function is then optimized, and this procedure suggests improved candidates 
    for the parameters $\theta$, starting from random or pre-trained initial values.
\end{enumerate}

The crucial point to emphasize of a VQA is that the PQC can be executed on current quantum
devices, while the computational demanding task of optimizing the objective function is
performed by a classical computer, which is far more reliable then current NISQ devices.
Therefore, the trademark of VQAs is that they use a quantum computer to estimate the cost function (or its gradient) 
while leveraging the power of classical optimizers to train the parameters: this is why they are called
\textit{hybrid quantum-classical} algorithms.\\
Many different VQAs have been designed to tackle standard machine learning tasks, such
as classification, regression or optimization.
CITAZIONE

\subsection{Parametrized Quantum Circuit}


Parameterized Quantum Circuits are the core of every VQA.\\
Consequently, it is crucial to design circuits that are not only \textit{easy to train} but also capable of 
\textit{generalizing} from the training data and sufficiently \textit{expressive} to identify an optimal solution 
for the given task.\\

A PQC, also known as \textit{quantum neural network}, can be expressed as a series of L parametrized unitaries:

\begin{equation}
    U(\theta) = \prod_{i=1}^L U_i(\theta_i)
\label{Eq:definition-PQC}
\end{equation}

where $\theta = (\theta_1, ..., \theta_L)$ is a set of tunable parameters.
This parameters are updated by optimizing a loss function:

\begin{comment}
\[
    \ell_{\theta}(\rho, O) = \text{Tr}\left[\rho(\theta)O\right]
    \qquad
    \rightarrow
    \qquad
    \argmin_{\theta} l_{\theta}(\rho, O)
\]
\end{comment}


More general loss functions exist, but this case is already relevant.\\
The main properties of a PQC are \textit{expressivity}~\cite{hubregtsen2020, Sim_2019, Bravo_Prieto_2020, 
Wu_2021, Herman_2023, Haug_2021, Holmes_2022}, \textit{generalization}~\cite{Caro_2020, Abbas_2021, Banchi_2021,
Bu_2023, bu2021, Bu_2022, Du_2022, Peters_2023, Caro_2023} and \textit{trainability}~\cite{McClean_2018, 
Cerezo_2021, Arrasmith_2021, Kim_2021, Wang_2021, Pesah_2021, marrero2021, Larocca_2023}.

\paragraph{Trainability}

As in classical machine learning, variational quantum algorithms can suffer from the problem of vanishing 
gradient, or in QML terms \textit{Barren Plateaus} (BP).
A common definition of BP for VQAs is the following:

\begin{defn*}[Barren Plateaus]
    A VQA exhibits a BP if its loss function, or its gradients, concentrate exponentially about their mean 
    in the number of qubits n
    \footnote[1]{The two definitions of Barren Plateaus — one involving gradients and the other 
    involving the loss function — are not equivalent. The definition based on the loss function is 
    unambiguous, whereas the definition in terms of partial derivatives can be less clear. The ambiguity 
    arises because some partial derivatives might be exponentially concentrated while others are not. In such 
    cases, the loss function would not exhibit exponential concentration, indicating that the two definitions 
    are not equivalent. They become equivalent only when \textit{all} the gradients are exponentially concentrated 
    \cite{Arrasmith_2022}.}.
\end{defn*}

This means that, as the number of qubits n increases, the gradients of the loss function (or the loss function itself) 
tend to concentrate around an average value with increasing probability.    
The average value around which the gradient must concentrate in order to exhibit BP must be very 
small\footnote[1]{\cite{McClean_2018} demonstrated that, under broad conditions, random parameterized quantum 
circuits have an average gradient value of zero.}.
In fact, if the gradient were concentrated around a non-infinitesimal value, it would mean that the 
algorithm is still learning.
Intuitively, this means that the optimization landscape is mostly flat and featureless and
that slightly changing the model’s parameters $\theta$ results in only an exponentially small change in the loss
function value or its gradient.\\

The probabilistic nature of Quantum Mechanics forces us to measured a finite amount of times N the PQC to determine
the average value of an observable.
The statistical uncertainty related to the average value is inversely proportional to $\sqrt{N}$.
Therefore, if the loss function has an exponential concentration (which means that the changes of the loss function 
from one epoch to the following are exponentially suppressed), we need an exponential amount of shots to resolve this
changes, rendering the algorithm inefficient and non-scalable.
 

Two main types of BPs exist:

\begin{comment}
\begin{enumerate}
    \item \textbf{Probabilistic concentration and narrow gorges}.\\
    The most common type of BP is known as \textit{probabilistic concentration}.

    \begin{equation}
        Var_{\theta} \[ \ell_{\theta}(\rho, O) \] or Var_{\theta} \[ \partial_{\mu} \ell_{\theta}(\rho, O) \] \in \mathcal{O}(\frac{1}{b^n})
    \end{equation}

    where $b > 1$. Chebyshev's inequality implies for any $\delta > 0$:

    \begin{align}
        Pr(|\ell_{\theta}(\rho, O) - \mathbb{E}_{\theta}(\ell_{\theta}(\rho, O))| \ge \delta) \in \mathcal{O}(\frac{1}{b^n})\\
        Pr(|\partial_{\mu} \ell_{\theta}(\rho, O) - \mathbb{E}_{\theta}(\partial_{\mu} \ell_{\theta}(\rho, O))| \ge \delta) \in \mathcal{O}(\frac{1}{b^n})\\
    \end{align}

    which means that the probability that the loss deviates from its mean is exponentially small.\\
    Probabilistic BP imply a mostly flat landscape with some \textit{narrow gorge}.
    
    \item \textbf{Deterministic concentration}.\\
    A deterministic BP arises when $\forall \theta$:
    \begin{align}
        |\ell_{\theta}(\rho, O) - \mathbb{E}_{\theta}(\ell_{\theta}(\rho, O))| \in \mathcal{O}(\frac{1}{b^n})\\
        |\partial_{\mu} \ell_{\theta}(\rho, O) - \mathbb{E}_{\theta}(\partial_{\mu} \ell_{\theta}(\rho, O))| \in \mathcal{O}(\frac{1}{b^n})\\
    \end{align}

    where $b > 1$.
    Deterministic BP do not even admit narrow gorges.
\end{enumerate}
\end{comment}

A possible method to determine if the loss or the gradients are concentrating was introduced by \cite{McClean_2018}.
They randomly sampled parameters and computed how the variance scales with the number of qubits 
(figure \ref{figure:McClean}).
In particular, this paper showed that for a large class of random quantum circuits the average value of the gradient of the objective 
function is zero and the probability that any given instance of such a random circuit deviates from this average 
value by a small constant $\epsilon$ is exponentially small in the number of qubits.
The region where the average value of the gradient is zero does not correspond to a local minimum, but rather an 
exponentially large plateau in the landscape of the parameters.\\

\begin{figure}
    \centering
    \includegraphics[width=.45\textwidth]{sections/chapters/Chapter4/Images/ProjectionGradientVar.pdf}
    \includegraphics[width=.45\textwidth]{sections/chapters/Chapter4/Images/ProjectionGradientVarLayers.pdf}
    \caption{These are the original images from the paper by \cite{McClean_2018}. 
    The image on the left shows the variance of the gradient of a two-local Pauli term plotted as a function of 
    the number of qubits: an exponential decay is observed as a function of the number 
    of qubits for both the expected value and its spread.
    The image on the right shows the variance of the gradient of a two-local Pauli term plotted as a function of the 
    number of layers. The different lines correspond to all even numbers of qubits between 
    2 and 24, with 2 qubits being the top line, and the rest being ordered by qubit number. This shows 
    the convergence of the variance as a function of the number of layers to a fixed value determined by the 
    number of qubits.
    }
    \label{figure:McClean}
\end{figure}

Other possible quantities have introduced to study if a BP occurs: \textit{landscape's information content} \cite{P_rez_Salinas_2024}, 
\textit{local state's entanglement} \cite{Sack_2022}, \textit{landscape's Fourier coefficients} \cite{okumura2023, Nemkov_2023}.

The final remark on trainability is to discuss the origin of BPs:

\begin{itemize}
    \item \textbf{Curse of dimensionality}.\\
    \item \textbf{Circuit expressiveness}.\\
    \item 
\end{itemize}

\paragraph{Generalization}


\paragraph{Expressivity}



\section{Quantum Machine Learning}

\subsection{Linear quantum models: quantum classifiers and kernel methods}

\subsection{Data reuploading models and Quantum Neural Network}

\subsection{Deriving the Fourier expansion}

\subsection{A single-qubit data reuploading circuit}

\subsection{Quantum Neural Networks}

\subsection{Generalization of QML models}

\subsection{The power of quantum machine learning}







