
The future investigations regarding the \textit{block re-uploading} algorithm will be numerous:

\begin{itemize}
    \item \textbf{Architectures}.\\
    Thus far we have mostly investigated the following architecture:

    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.65]{sections/chapters/Chapter6/Images/Two-Layers-A/two_layer.pdf}
        \caption{A four qubits and two layers \textit{block re-uploading} architecture. Each layer has three 
        components: an embedding circuit, an entanglement structure and a pooling circuit.
        Layers are separated by another entanglement structure.}
    \end{figure}

    In the future, we plan to explore simpler architectures before advancing to the more 
    complex \textit{block re-uploading} architecture. 
    Our plan will be a bottom-up approach, where we will \textit{progressively add new features} to the baseline model, 
    which has only the \textit{embedding} component (figure).
    Therefore, after studying the baseline model, we will explore the architectures with the 
    \textit{embedding} and \textit{pooling} components and finally we will study the architectures 
    with the \textit{embedding}, \textit{pooling} and \textit{entanglement} components.
    This step-by-step investigation will 
    help us assess the effectiveness of each component added to the baseline \textit{embedding} model.

    \begin{figure}[h]
        \centering
        \begin{subfigure}[b]{0.45\textwidth}
            \includegraphics[scale=0.58]{sections/chapters/Chapter6/Images/Architectures/embedding.pdf}
        \caption*{Baseline model.}
        \end{subfigure}
        \begin{subfigure}[b]{0.45\textwidth}
            \includegraphics[scale=0.62]{sections/chapters/Chapter6/Images/Architectures/embedding_pooling.pdf}
        \caption*{Baseline model with pooling feature.}
        \end{subfigure}
        \begin{subfigure}[b]{\textwidth}
            \centering
            \includegraphics[scale=0.68]{sections/chapters/Chapter6/Images/Two-Layers-B/two_layers_B.pdf}
        \caption*{Baseline model with pooling and entanglement features.}
        \end{subfigure}
    \end{figure}

    \item \textbf{Initialization}.\\
    Another interesting line of research could be to test different parameters initialization (thus far
    we tested only gaussian and uniform) to increase the architectures' \textit{layers budget}.
    
    \item \textbf{Warm-Up start}.\\
    So far, we have trained each architecture from scratch. In future experiments, we plan to explore 
    a new training strategy where an architecture with $L-1$ layers is first trained, and then, 
    for training an architecture with $L$ layers, we initialize the parameters of the first $L-1$ 
    layers using the trained parameters from the $L-1$ layer architecture.\\
    This a technique of \textit{transfer learning}.


    \item \textbf{Are shallow architectures contained in deep architectures?}.\\
    We would like to answer to this question for two different quantum re-uploading models:
    quantum re-uploading models where data and parameters are embedded in the same gates\footnote[1]{The 
    \textit{block re-uploading} architecture belongs to this first category} (which we will call the \textit{mixed}
    case), quantum re-uploading models where data and parameters are separated (which we will call the 
    \textit{separate} case).\\

    \begin{figure}[h]
        \centering
        \begin{subfigure}[b]{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{sections/chapters/Chapter6/Images/Separate-Data-Parameters/separate_data_param.pdf}
            \caption*{Separate quantum re-uploading model.}
        \end{subfigure}
        \begin{subfigure}[b]{0.45\textwidth}
            \centering
            \includegraphics[width=0.9\textwidth]{sections/chapters/Chapter6/Images/Mixed-Arch/mixed.pdf}
            \caption*{Mixed quantum re-uploading model.}
        \end{subfigure}
    \end{figure}

    In the \textit{mixed} case, it is straightforward to demonstrate that deep architectures contain shallow ones. 
    This can be achieved simply by initializing the parameters of the additional layers to zero.\\
    However, in the \textit{separate} case, the situation becomes more complex, especially when we do not permit 
    "trivial data embeddings". One approach to discuss the relation between deep and shallow architectures is 
    through the use of \textit{truncated Fourier series}. Each quantum model can be represented as a truncated 
    Fourier series \cite{Schuld_2021}, where the data embedding influences the frequencies, and the variational component adjusts 
    the coefficients.
    Therefore, to prove that deep architectures contain shallow architectures in this context, 
    we could aim to show that the truncated Fourier series representation of a deep architecture can 
    be reduced to that of a shallow architecture.\\
    Another possible (but less sophisticated) approach could be the following: if we consider for instance 
    a 2-layers mixed architecture (figure ) we could prove that it contains a 1-layer architecture by adjusting the 
    parameters $\theta_2$, 
    such that the quantum state $|\psi \rangle$ output from the first layer becomes an eigenvector 
    of the second layer.\\
    The crucial observation is that the data-embedding gates of the second layer, $U_2(x)$, 
    will first act on $|\psi \rangle$. Therefore, we need to adjust the parameters $\theta_2$ 
    to account for the effect of $U_2(x)$.\\

    \item \textbf{What is the relation depth-width of an architecture?}.\\
    Tensor Networks allow us to transform a quantum circuit with a certain topology 
    (by topology I mean the number of qubits and number of layers) into a quantum circuit with a 
    different topology\footnote[1]{Every quantum circuit can be mapped to a tensor network. While this is 
    always true, the reverse is not: the statement \textit{every tensor network is a quantum 
    circuit} is false. This is because quantum circuits impose specific constraints on tensor 
    networks, such as requiring the gates to be unitary matrices.}. Therefore, we could use them as a tool to prove some kind of equivalence 
    relationship between the layers of a circuit and the depth of a circuit.

    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.85]{sections/chapters/Chapter6/Images/Tensor-Networks/TN-properties.pdf}
        \caption{Some useful properties of tensor networks to transform a quantum circuit with a certain 
        topology into a quantum circuit with a different topology.}
        \label{fig:tensor-prop}
    \end{figure}

    For example, consider a 1-qubit, 4-layer circuit (see figure \ref{fig:1q-4l}). By leveraging 
    the properties of tensor networks, we can transform this circuit into a 2-qubit, 4-layer circuit 
    (see figure \ref{fig:2q-4l}). Further, we can bend it once more to obtain a 4-qubit, 
    1-layer circuit (see figure \ref{fig:4q-1l}). This heuristic argument suggests a potential path 
    for demonstrating that \textit{a deep circuit is equivalent to a wide circuit}. 
    More broadly, it implies a relationship between the depth and width of a 
    circuit\footnote[1]{Figures \ref{fig:1q-4l}, \ref{fig:2q-4l}, and \ref{fig:4q-1l} indicate 
    that an architecture with a certain depth $d$ can be equivalent to an architecture with width 
    $w = d$.}.\\

    \begin{figure}[h]
        \centering
        \begin{subfigure}[b]{\textwidth}
            \includegraphics[scale=0.5]{sections/chapters/Chapter6/Images/Tensor-Networks/1qubit4layers.pdf}
        \caption{This is a 1-qubit 4-layers quantum circuit. The blue circle and green circle represents two
        different components of a single layer. The red triangle represents the input state, whereas the 
        gray semi-circle represents the measurement.}
        \label{fig:1q-4l}
        \end{subfigure}
        \\[3ex]
        \begin{subfigure}[b]{\textwidth}
        \centering
            \includegraphics[scale=0.5]{sections/chapters/Chapter6/Images/Tensor-Networks/2layers2qubits.pdf}
        \caption{By bending the 1-qubit 4-layers quantum circuit, we obtain a 2-qubits 2-layers circuit. If 
        we use tensor networks' properties show in figure \ref{fig:tensor-prop} we obtain a 2-qubits 
        2-layers circuit.}
        \label{fig:2q-2l}
        \end{subfigure}
        \\[3ex]
        \begin{subfigure}[b]{\textwidth}
        \centering
            \includegraphics[scale=0.5]{sections/chapters/Chapter6/Images/Tensor-Networks/4qubits1layer.pdf}
        \caption{By bending the 2-qubit 2-layers quantum circuit, we obtain a 4-qubits 1-layer circuit.}
        \label{fig:4q-1l}
        \end{subfigure}
        \caption{This figure shows how we can transform a 1-qubit 4-layers architecture in 4-qubits 1-layer 
        circuit by exploiting tensor networks' properties.}
    \end{figure}

    Therefore, by using Tensor Networks we could prove a relation width-depth relation
    for quantum circuits. 
    
    \item \textbf{Are narrow architectures contained in wide architectures?}.\\
    We still need to consider how best to approach this question. However, after exploring the 
    relationships between depth and width as well as shallow and deep architectures, it seems logical to 
    now examine the relationship between narrow and wide architectures.
    
\end{itemize}