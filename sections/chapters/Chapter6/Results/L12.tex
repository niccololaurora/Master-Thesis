\begin{figure}[h]
    \centering
    \includegraphics[scale=0.3]{sections/chapters/Chapter6/Images/Heatmaps/Local-Heatmaps/Heatmap-Both-12x12-L.pdf}
    \caption{This heatmap shows the training and validation accuracy for architectures 
    trained on the $12\times12$ down-scaled MNIST digits and fashion dataset using a local Pauli Z.}
    \label{fig:heatmap-12x12-L}
\end{figure}

By enlarging the images, we observe a consistent trend across all categories 
(deep-narrow, shallow-wide, proportionate): the training accuracy remains basically the same, whereas we 
can notice an increasing presence of overfitting.
We can observe that the general behaviour of the validation accuracy seen with $8\times8$ images is 
similarly observed with $12\times12$ images, but \textit{shifted to the left by one or two layers}\footnote[1]{We are comparing only the local case. 
By general behaviour we intend that as the architecture becomes deeper overfitting becomes more 
pronounced.}. This shift 
occurs because the increased number of parameters in each layer makes the model more prone to overfitting.\\

By looking at figure \ref{fig:heatmap-12x12-L}, we can distinguish 
three different behaviours:

\begin{itemize}
    \item \textbf{Deep-Narrow}: $8\times8$ vs $12\times12$.\\
    Overfitting is more pronounced for deep-narrow architectures.\\
    In both the $8\times8$ and $12\times12$ cases, overfitting worsens as the number of layers increases. \\
    Training accuracy remains stable for $8\times8$ images, but for $12\times12$ images, 
    it decreases with the addition of more layers.
    \item \textbf{Shallow-Wide}: $8\times8$ vs $12\times12$.\\
    In the $8\times8$ case we noticed the drastic drop for both the validation and training accuracy for 
    architectures wider than 8 qubits, so since for the $12\times12$ we did not train architectures wider than 
    8 qubits we cannot conclude that we would see the same drastic drop.
    However, since deep-narrow and proportionate architectures in the $12\times12$ case did not show 
    significant differences with the $8\times8$ case we can assume that even Shallow-Wide architectures will
    not show significant differences.\\
    The effect of distributing information across more qubits in shallow architectures is particularly 
    evident in the $12\times12$ case. For instance, when examining 2-layer architectures vertically, 
    it's clear that spreading the information across more qubits significantly enhances both training and 
    validation accuracy.
    \item \textbf{Proportionate}: $8\times8$ vs $12\times12$.\\
    Again overfitting tends to disappear. \\
 \end{itemize}