\begin{figure}[h]
    \centering
    \includegraphics[scale=0.3]{sections/chapters/Chapter6/Images/Heatmaps/Local-Heatmaps/Heatmap-Digits-16-18-L.pdf}
    \caption{This heatmap shows the training and validation accuracy for architectures for the $16\times16$ and $18\times18$ down-scaled MNIST digits
     dataset using a local Pauli Z.
     Some training sessions run for 150 epochs (e.g., $16 \times 16$: 2 qubits, $18 \times 18$: 
     1 qubit, 3 qubits), while others are extended to 200 epochs.}
    \label{fig:heatmap-14x14-L}
\end{figure}

By looking at figure \ref{fig:heatmap-14x14-L}, we can distinguish 
three different behaviours: