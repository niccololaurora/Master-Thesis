\begin{figure}[h]
    \centering
    \includegraphics[scale=0.3]{sections/chapters/Chapter6/Images/Heatmaps/Global-Heatmaps/Heatmap-Both-12x12-G.pdf}
    \caption{This heatmap shows the training and validation accuracy for architectures with 1-15 qubits and
    1-6 layers for the $12\times12$ down-scaled MNIST digits and fashion dataset using a global Pauli Z.}
    \label{fig:heatmap-12x12-G}
\end{figure}

By looking at figure \ref{fig:heatmap-12x12-G}, we can distinguish 
three different behaviours:

\begin{itemize}
    \item \textbf{Deep-Narrow}: $8 \times 8$ vs $12 \times 12$.\\
    
    \item \textbf{Shallow-Wide}: $8 \times 8$ vs $12 \times 12$.\\
    
    \item \textbf{Proportionate}: $8 \times 8$ vs $12 \times 12$.\\
    
 \end{itemize}