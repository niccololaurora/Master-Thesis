\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{sections/chapters/Chapter6/Images/Trainability/variance-qubits-layer-local-uniform.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{sections/chapters/Chapter6/Images/Trainability/variance-qubits-layer-global-uniform.pdf}
    \end{subfigure}
    \label{fig:local-uniform}
\end{figure}




We investigated the \textit{trainability} of the \textit{block re-uploading} architecture. 
The plots in figure \ref{fig:local-uniform} were produced following the procedure introduced by 
the foundational paper by \cite{McClean_2018}:

\begin{itemize}
    \item We fixed the size of the images: $8\times8$;
    \item We fixed the number of layers: 1, 10, 15, 20, 40, 60, 80, 100;
    \item We fixed the number of qubits: 1, 2, 3, ..., 16;
    \item We sampled N (50) models, which means that we sampled N different times the parameters 
    $\bm{\theta}$ of the model (uniform and gaussian initialization);
    \item We computed the absolute gradient of the loss function $|\nabla_{\bm{\theta}}J|$\footnote[1]{
        Alternatively, we could have computed only the loss function. As previously discussed, 
        barren plateaus can be defined either through the loss function or its gradient. 
        This approach would have been computationally more efficient.
    };
    \item We computed the mean value of the absolute gradient $\langle |\nabla_{\bm{\theta}}J| \rangle$;
\end{itemize}



\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{sections/chapters/Chapter6/Images/Trainability/variance-qubits-layer-local-uniform.pdf}
    \caption{This figure shows}
    \label{fig:local-gauss}
\end{figure}